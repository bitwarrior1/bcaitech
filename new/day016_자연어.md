# 강의 

### 자연어 분야 3가지

* 자연어처리
* 텍스트마이닝
* 정보획득

### 자연어 키워드

* Word2Vec, Glove   
텍스트를 Sequence데이터로 보고 벡터로 나타내는 방법

* RNN  
LSTM, GRU를 이용한 NLP 아키텍쳐

* Transformer   
  * attention모듈을 사용한 모델. 최근 성능향상을 이끈 방법. 다른 분야에도 적용가능.  
  * 이전에는 각 필요와 상황에 맞게 다른 모델들을 사용했는데 Transformer모델은 큰 수정없이 transfer learning을 통해서 다른 task도 해결 가능.
  * self-supervised, pretrained model과 관련 (무엇?)
  * 최신 자연어 처리는 막대한 리소스 필요 -> 소수기관들에 의해서 진행 (Google, OpenAI...)  


### 텍스트 표현 방법들

#### 1. Bag of Words

1. 등장하는 단어들을 unique하게 정리
2. 단어을 카테고리 하나라고 보며 one-hot-vector 형태로 표현
3. 문장을 단어들인 one-hot-vector들의 합으로 표현

* 모든 단어의 one hot vector의 크기가 동일하므로 문제가 된다.  
(유클리디안 거리 √2, 코사인 유사도 0)

#### 2. Naive Bayes Classifier

* Bag of Words을 이용하는 방법이다

* Bayes Rule

<img src="https://github.com/bitwarrior1/bcaitech/blob/main/new/img/board1.png" width=700 height=400 />


* 문제점  
어떤 클래스 내에 없는 단어를 가진 문장이 주어질 경우 이 문장이 그 클래스에 속할 확률이 0으로 계산된다.   
(클래스에 포함될 확률이 각 단어의 조건부확률의 곱으로 계산되므로)   
방지하기 위해서 분모, 분자에 1추가하는 것과 같은 방법들을 사용한다.
