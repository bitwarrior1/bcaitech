# 강의 

### 자연어 분야 3가지

* 자연어처리
* 텍스트마이닝
* 정보획득

### 자연어 키워드

* Word2Vec, Glove   
텍스트를 Sequence데이터로 보고 벡터로 나타내는 방법

* RNN  
LSTM, GRU를 이용한 NLP 아키텍쳐

* Transformer   
  * attention모듈을 사용한 모델. 최근 성능향상을 이끈 방법. 다른 분야에도 적용가능.  
  * 이전에는 각 필요와 상황에 맞게 다른 모델들을 사용했는데 Transformer모델은 큰 수정없이 transfer learning을 통해서 다른 task도 해결 가능.
  * self-supervised, pretrained model과 관련 (무엇?)
  * 최신 자연어 처리는 막대한 리소스 필요 -> 소수기관들에 의해서 진행 (Google, OpenAI...)  


### 텍스트 표현 방법들

#### 1. Bag of Words

1. 등장하는 단어들을 unique하게 정리
2. 단어을 카테고리 하나라고 보며 one-hot-vector 형태로 표현
3. 문장을 단어들인 one-hot-vector들의 합으로 표현

* 모든 단어의 one hot vector의 크기가 동일하므로 문제가 된다.  
(유클리디안 거리 √2, 코사인 유사도 0)

#### 2. Naive Bayes Classifier

* Bag of Words을 이용하는 방법이다  
* Bayes Rule 사용  

<img src="https://github.com/bitwarrior1/bcaitech/blob/main/new/img/board1.png" width=700 height=400 />

argmax P(d|c)P(c) 계산시 conditional independence assumption 방법 사용 (d->각 word)

<img src="https://github.com/bitwarrior1/bcaitech/blob/main/new/img/board2.png" width=700 height=100 />

* 문제점  
어떤 클래스 내에 없는 단어를 가진 문장이 주어질 경우 이 문장이 그 클래스에 속할 확률이 0으로 계산된다.   
(클래스에 포함될 확률이 각 단어의 조건부확률의 곱으로 계산되므로)   
방지하기 위해서 분모, 분자에 1추가하는 것과 같은 방법들을 사용한다.

### 3. Word Embedding

* 텍스트를 Vector로 표현하는 방법 통칭?.
* 비슷한 의미, 관련된 단어끼리 가까운 거리내에 있다.

### 4. Word2Vec

* word embedding 방법.
* 인접단어들이 서로 관련성이 높다고 가정한다.
* 용어 window. 어디까지 인접단어로 적용할지에 대한 범위. 
* 용어 lookup. one-hot-vector가 가중치행렬과 matrix multiplication할 때 '1'과 계산되는 행벡터를 그대로 읽어오는.
* 학습데이터는 (input, output)단어쌍으로 구성.  
  window가 1이라고하면 하나의 중심단어에 대하여 (주변단어1, 중심단어), (주변단어2, 중심단어)으로 학습데이터를 구성한다.  


1. Continuous Bag of Words  
  주변단어가 input, 중심단어가 output  
    * input layer에서 주변단어들이 가중치와의 연산 후, 그 결과값들의 평균을 내는 방식 사용.

2. Skip-gram  
  중심단어가 input, 주변단어가 output
    * 중심단어 * window 크기 * 2 를 적용해서 반복되는 input값을 만들어 output인 주변단어와 데이터단어쌍을 맞춘다.

* 같은 관계는 같은 벡터로 표현된다.
* embedding된 데이터가 단어간의 관계를 학습했다고 본다.  
  (단어벡터를 더하거나 뺀 후 가까운 벡터를 확인하면 학습된 관계를 확인가능하다.)

### 5. Glove

* co-occurrence를 사용한다.  
  co-occurrence? 단어가 다른 단어의 window 내에서 등장한 빈도
