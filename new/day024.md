# 강의

### 1. 정점 표현 학습 (=정점 임베딩)
* 그래프의 정점들을 **벡터**로 표현하는 것
* 정점 임베딩(embedding)이라고도 한다.
* 정점 임베딩은 벡터의 표현 그 자체를 의미하기도 한다.

* **정점 표현 학습의 이유**
    * 정점 임베딩의 결과로, 벡터 형태의 데이터를 위한 도구들을 그래프에도 적용할 수 있습니다

* 그래프에서의 정점간 **유사도**를 임베딩 공간에서도 “보존”하는 것을 목표로 합니다
* 임베딩 공간에서의 **유사도**로는 내적(Inner Product)를 사용합니다
  임베딩 공간에서의 𝑢와 𝑣의 유사도는 둘의 임베딩의 내적 𝒛𝒗⊺𝒛𝒖 = ||𝒛𝒖|| ⋅ ||𝒛𝒗|| ⋅ 𝒄𝒐𝒔 𝜽 입니다
  내적은 두 벡터가 클 수록, 그리고 같은 방향을 향할 수록 큰 값을 갖습니다

* 그래프에서 정점 사이의 유사성을 계산하는 방법에 따라 여러 접근법이 구분됨

### 2. 인접성 기반 접근법
* 두 정점의 간선이 존재할 때 1, 없을 때 0을 유사도 값으로 설정
* 인접행렬(adjacency)로 그래프 유사도 표현
* 손실함수

<img src="https://github.com/bitwarrior1/bcaitech/blob/main/new/img/loss_adjacency.png" width="700" height="250" />

* 한계
    * 단순하게 인접성만 판단해서 거리에 차이에 대한 개념이 없다

### 3. 거리/경로/중첩 기반 접근법
* 거리: 두 정점 사이 거리 기준 유사성
* 경로: 두 정점 사이의 경로의 수 기준 유사성
* 중첩: 두 정점이 공유하는 이웃의 수 기준 유사성
    * 자카드 유사도 혹은 Adamic Adar 점수: 
      * 이웃의 수 대신 비율을 사용한다.
      * 정점의 연결성(degree)를 가중치로 사용.
      * 연결성이 높다면(다른 이웃도 많음) 유사도가 낮게 나올 것


### 4. 임의보행 기반 접근법
* 임의보행이란 하나의 정점에서 이웃 중 하나를 균일한 확률로 선택해 이동을 반복하는 것을 말한다.
* 임의보행 방법에 따라 DeepWalk와 Node2Vec가 구분된다
* DeepWalk는 기본적인 임의보행 방법입니다.

<img src="https://github.com/bitwarrior1/bcaitech/blob/main/new/img/random_walk.png" />

* Node2Vec는 임의보행시 균일한 확률이 아니라 직전 정점의 거리를 기준으로 경우를 구분하여 차등적인 확률을 부여합니다.
* Node2Vec에서는 부여하는 확률에 따라서 다른 종류의 임베딩을 얻습니다.

### 손실함수근사
  * 임의보행 기법의 손실함수는 계산에 **정점의 수** 의 제곱에 비례하는 시간이 소요됩니다.
  * 근사식을 사용.
  <img src="https://github.com/bitwarrior1/bcaitech/blob/main/new/img/approximation_loss_function.png" width="700" height="300" />

### 5. 변환식(Transductive) 정점 표현 학습의 한계
  * 위의 방법들은 변환식 방법들로써 정점 임베딩 자체를 구하는 방식이다.
  * 인코딩함수를 구하는 방식, 귀납적(Inductive) 방법과 대조된다.
  * 변환식 임베딩 방법의 한계
    1) 학습이 진행된 이후에 추가된 정점에 대해서는 임베딩을 얻을 수 없습니다 (= 새로구해야함)
    2) 모든 정점에 대한 임베딩을 미리 계산하여 저장해두어야 합니다
    3) 정점이 속성(Attribute) 정보를 가진 경우에 이를 활용할 수 없습니다
  

---

# 강의 2

### 1. 추천시스템 기본 복습
* 내용 기반 추천, 협업 필터링 등
##### 1. 내용 기반
   1. 사용자 프로필을 만들때 상품 프로필을 사용함.(둘다 벡터)
   2. 사용자가 준 평점에 따라서 상품에 가중치 적용.
   3. 만들어진 사용자 프로필과 유사한 상품을 추천.  

   * 장점:다른 사용자가 필요없음... 등
   * 단점:상품의 특성 데이터가 필요, 과적합 등

##### 2. 협업 필터링
   다른 사용자들을 이용.  
   1. 사용자와 유사도가 높은 다른 사용자들을 구한다. 상관계수, correlation coefficient 이용.  
   2. 상품에 대해서 사용자 유사도를 가중치로 그 사용자들의 평점을 가중합한다.  
   3. 높은 예측평점의 상품을 추천.  

   * 장점:상품의 특성 데이터가 필요없다... 등  
   * 단점:새로운 상품에 대해 쓸수 없다... 등  

### 2. 넷플릭스 챌린지 소개
넷플릭스에서 추천시스템의 RMSE를 줄이는 대회를 염. 상금 100만불 > 10억!

### 3. 기본 잠재 인수 모형
UV decomposition 또는 SVD(유사한 측면을 가지는 수학적개념)라고도 불린다
#### Latent factor = 잠재인수
고정된 인수를 사용하는 것이 아닌 효과적인 인수를 학습하는 것을 목표로 합니다.
학습한 인수를 잠재 인수(Latent Factor)라 부릅니다.
  
* row 사용자 col 상품인 행렬 R이 있다고 할때
* 행렬곱 PQ로 R을 생성하는 P행렬(=사용자 임베딩 행렬) Q(=상품 임베딩 행렬) 학습시킨다.
* **과적합 방지(=훈련데이터의 Noise 제거)할 목적으로** regularization을 LOSS함수에 적용한다.

<img src="https://github.com/bitwarrior1/bcaitech/blob/main/new/img/regularization_in_embedding.png" width="700" height="700" />

* SGD사용: 경사하강법을 빠르게 적용하기 위해서

### 4. 고급 잠재 인수 모형
* 사용자 편향 고려: 해당사용자 평균평점 - 전체사용자 평균평점
* 상품 편향 고려: 해당상품 평균평점 - 전체상품 평균평점
* 시간적 편향을 고려
   * 구체적으로 사용자 편향과 상품 편향을 시간에 따른 함수로 가정합니다  
* p,q의 내적이 R을 예측하는 것이 아닌 상호작용을 의미
<img src="https://github.com/bitwarrior1/bcaitech/blob/main/new/img/loss_latent_factor.png" width="800" height="100" />


### 5. 넷플릭스 챌린지의 결과
마지막에 ensemble로 성공...

### 6. 실습: Surprise 라이브러리와 잠재 인수 모형의 활용
