# 강의
## 8강 - Features

### Feature importance
* 각 feature가 얼마나 중요한지 판단 지표
1. model-specific
    * 모델에 종속적
    * lightGBM에 경우 split, gain 두가지 평가방법이 있다
      * split: (default), 모델을 만드는데 해당 feature를 얼마나 많이 사용하는가? 와 관련
      * gain: feature를 사용해서 얻는 gain과 관련
    * XGboost, Catboost에 경우 split, gain과 유사한 지표들이 있으며 추가적으로 다른 지표도 있다고 한다.

2. model-agnostic
    * 모델에 독립적
    * **Permutation feature importance**
    * feature들을 random shuffling하고 학습후 error를 확인한다. 중요한 지표가 사라졌다면 오류가 증가할것이다. 중요하지 않는 지표는 오류가 동일할것

* **Permutation 방법이 많이 쓰인다**, 실제점수를 확인하는 강점
* model-specific은 거의 참고용이라고 볼수 있다

### Feature selection
* 중요한 feature를 선택하는 방법
1. Filter method
    * feature의 상관관계를 계산한다
    * 간단함, 빠름
    * 대게 전처리시 사용한다
    * 상관관계가 높은 feature들 : 중복 가능성
    * 작은 variance를 가진 feature : 무의미한 feature 가능성

2. Wrapper method
    * 예측모델을 사용한다
    * feature의 subset을 평가한다
    
3. Embedded method
    * 위의 filter, wrapper 방법을 결합이라고 볼수 있다
    * warpper method를 진행하면서 filter method를 학습알고리즘에 포함시킨다
 
 #### 실무
 * 현업에서는 feature importance는 permutation을 사용한다.
 * 단순, 기본 통계량을 사용한 feature를 많이 만들어서 테스트한다.
 * feature를 함부로 삭제하지 않는다. 
 * 정성적 평가가 가능한 feature를 좋아한다. 
