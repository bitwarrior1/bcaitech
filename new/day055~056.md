# 강의
## 8강 - Features

### Feature importance
* 각 feature가 얼마나 중요한지 판단 지표
1. model-specific
    * 모델에 종속적
    * lightGBM에 경우 split, gain 두가지 평가방법이 있다
      * split: (default), 모델을 만드는데 해당 feature를 얼마나 많이 사용하는가? 와 관련
      * gain: feature를 사용해서 얻는 gain과 관련
    * XGboost, Catboost에 경우 split, gain과 유사한 지표들이 있으며 추가적으로 다른 지표도 있다고 한다.

2. model-agnostic
    * 모델에 독립적
    * **Permutation feature importance**
    * feature들을 random shuffling하고 학습후 error를 확인한다. 중요한 지표가 사라졌다면 오류가 증가할것이다. 중요하지 않는 지표는 오류가 동일할것

* **Permutation 방법이 많이 쓰인다**, 실제점수를 확인하는 강점
* model-specific은 거의 참고용이라고 볼수 있다

### Feature selection
* 중요한 feature를 선택하는 방법
1. Filter method
    * feature의 상관관계를 계산한다
    * 간단함, 빠름
    * 대게 전처리시 사용한다
    * 상관관계가 높은 feature들 : 중복 가능성
    * 작은 variance를 가진 feature : 무의미한 feature 가능성

2. Wrapper method
    * 예측모델을 사용한다
    * feature의 subset을 평가한다
    
3. Embedded method
    * 위의 filter, wrapper 방법을 결합이라고 볼수 있다
    * warpper method를 진행하면서 filter method를 학습알고리즘에 포함시킨다
 
 #### 실무
 * 현업에서는 feature importance는 permutation을 사용한다.
 * 단순, 기본 통계량을 사용한 feature를 많이 만들어서 테스트한다.
 * feature를 함부로 삭제하지 않는다. 
 * 정성적 평가가 가능한 feature를 좋아한다. 


## 9강 - 하이퍼파라미터 튜닝
### 방법
1. manual
2. grid
   * 자원이 많이 든다
3. random
   * grid보다 자원, 성능 측면에서 모두 좋다
4. bayesian
   * 많이 쓴다
   * 이전 탐색 내용을 바탕으로 탐색 + random

### 라이브러리 Optuna
* 일부 진행 후 끊고 이어서 탐색 가능

## 10강 - Ensemble
* 하나의 알고리즘이 모든 데이터(문제)에 월등할 수는 없으므로
* 오버피팅감소
* 성능향상

* 기법
   * Babbing
      * bootstrap + aggregation
      * bootstrap은 중복으로 뽑을 수도 있는 샘플링 방법이다.(with replacement) 
      * 중복없는 샘플링 방법은 pasting (거의 안씀)
   * Voting
      * hard: 결과 다수결
      * soft: 평균 확률을 사용
   * Boosting
      * 순차적인 앙상블
      * 하나의 모델학습 후 샘플링 가중치 적용후 다음 모델 반복
      * 성능 좋다, 속도 느리다
      * 오버피팅 가능성 상승
   * Stacking
      * 서브 모델들의 출력이 입력값이 되는 meta 모델을 사용
      * 서브모델은 서로 다른 알고리즘을 사용
      * 시간이 오래걸린다
      * 현실적으로 유용한가?

## Decision Tree
* Impurity (불순도)
1. Entropy
2. Gini Index
      * 불순도를 측정하는 지표로서 통계적 분산정도를 정량화해서 표현한 값
      * 결정트리의 분할 기준으로 사용
      * 예시)앞 동전3개, 뒤 동전2개
      * 1 - (3/5)^2 - (2/5)^2 = 0.48

2. information gain
   * Information gain is calculated by comparing the entropy of the dataset before and after a transformation.

# 오피스아워
강의와 겹치는 주제의 내용은 제외  

* 데이터 통찰 강조, 경험을 많이 쌓아야한다.
* 질문하고 실행해서 확인하는 과정을.
* 모델별 중요 파라미터가 다르다
* 정형데이터에도 딥러닝의 시도가 나오고 
