## 강의  

* 선형회귀분석  
데이터를 가장 잘 설명하는 선형모델을 찾는 것.

* 미분   
변수의 움직에 따른 함수값의 변화를 측정하기 위한 도구.   
sympy 패키지의 diff 사용  


## 경사 하강법 Gradient Descent

* 함수의 값을 최소화 시키는 방법

* 함수를 변수에 대해 미분하고 기존변수에 미분값을 빼는 방식으로 최소값으로 이동시킨다.

* 변수가 여러개일 때는 편미분한다. 즉 모든 d차원의 각 변수에 대해서 미분하여 d개 만큼의 미분식을 얻는다.  
  이 미분식들을 계산하여 벡터형태로 모은 것을 gradient 벡터라하고 변수가 하나일 때와 비슷하게 기존변수벡터에 gradient 벡터 값을 빼준다.  
nabla, <img src="https://render.githubusercontent.com/render/math?math=\nabla"> 기호는 gradient 벡터를 나타낸다.  

* 알고리즘 수식  
    <img src="https://github.com/bitwarrior1/bcaitech/blob/main/new/img/gradient.png"  width="300" height="50">


## 확률적 경사 하강법 Stochastic Gradient Descent

* 하나의 데이터를 사용하거나, 일부 데이터(mini batch)를 사용한다. 

* GD에 비해서 연산량이 매우 적다.

* 선형 & 비선형 모델 모두에서 사용할 수 있다.
  * 모든 데이터를 사용하는 GD는 local minimum에 빠질 수 있어서 일반적으로 사용하지 않는다.  
  * SGD가 local minimum에 빠지지 않는 방법은 경사하강할 때마다 일부의 데이터만 사용하므로 목적식이 바뀌기 때문이다.
  


## Further Question  

<img src="https://github.com/bitwarrior1/bcaitech/blob/main/new/img/partial differentiation.png"  width="600" height="200">

* 첫번째 줄 수식은 gradient 벡터이다.
* 두번째 줄 수식이 k번째 변수에 대해서 편미분한 것이다.
* 두번째 줄 수식을 정리하면 세번째 줄 수식이 된다. 이 과정을 풀어쓰면 

<img src="https://github.com/bitwarrior1/bcaitech/blob/main/new/img/latex1.png"  width="450" height="70">
<img src="https://github.com/bitwarrior1/bcaitech/blob/main/new/img/latex2_LI.jpg"  width="250" height="60">

* 위를 정리하면 세번째 줄 수식과 동일하다.
* 이제 모든 d에 대해서 편미분을 구한뒤 gradient 벡터를 구하면
<img src="https://github.com/bitwarrior1/bcaitech/blob/main/new/img/gradient vector.png"  width="450" height="100">

* L2노름의 제곱을 목적식으로 사용해서 최적화해도 괜찮다. 오히려 계산이 간단해진다.
