## 강의

* pandas  
  * tabular 형태의 데이터를 다루는데 유용한 라이브러리  
  * [jupyter에서 연습]()

* Activation function, 활성화 함수
  * 비선형 함수  
  * 선형모델과 결합하여 비선형 패턴을 학습하도록 한다.  
  * 선형모델의 출력값을 입력값으로 갖는다.  
  * 신경망 하나의 layer는 [입력 -> 선형모델 -> Activation Function -> 출력] 으로 이루어진다.  
  * sigmoid, tanh, ReLu가 대표적이다. 딥러닝에서는 ReLu가 쓰인다.  

* 신경망 층의 깊이
  깊이가 깊을수록 필요한 뉴런(노드)의 개수가 빨리 줄어든다.  
  그러나 학습에는 깊은 신경망이 더 어렵다.


## 오차역전파, backpropagation

신경망 가중치행렬(W) 수정 방법.  
미분의 chain rule을 통해서 뒤에서 부터 앞으로 계산이 진행된다.  
__추가학습__



## Further Question
* 분류 문제에서 softmax 함수가 사용되는 이유가 뭘까요??  
  * 지금까지 배운 방법(회귀?)으로는 결과값이 하나의 실수로 나오는데 분류문제는 그렇지 않다.  
  예를들어, 3가지 클래스(선택지)가 있으면 각 클래스마다 가중치 행렬을 갖고 있고 결과또한 각 클래스마다 나오므로 3개가 나온다.  
  그리고 3가지 결과를 softmax함수를 통해서 확률로 바꾸고 가장 큰 확률의 클래스로 분류한다
  
* softmax 함수의 결과값을 분류 모델의 학습에 어떤식으로 사용할 수 있을까요?
  * softmax의 loss function은 cross entropy이다.    
  실제 클래스을 낮은 확률로 예측하면 오차의 값이 커진다.    
  반대로 실제 클래스를 높은 확률로 예측하면 오차의 값이 작아진다.  
  <img src="https://github.com/bitwarrior1/bcaitech/blob/main/new/img/cross%20entropy%20loss%20function.png" width="400" height="200">
  
  * 위의 이미지는 하나의 데이터에 대한 loss를 구하는 것을 보여준다.
  
  * 임의의 j클래스의 gradient벡터는 아래과 같다고 한다... __(어떻게 유도?)__
  <img src="https://github.com/bitwarrior1/bcaitech/blob/main/new/img/cross_entropy_gradient_vector.png" width="350" height="70">
    
