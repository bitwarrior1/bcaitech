# 강의

1. 네트워크를 깊게 쌓으면 좋다.
  * 큰 receptive field
  * More capacity and non-linearity (???)

2. 좋은데 문제가 생긴다
  * 계산 규모가 커짐
  * gradient exploding / vanishing 문제
  * degradation : 학습 하락, 오류 증가

### gooLeNet
  * Inception Block이라고하는 여러 kernel_size를 가진 필터를 사용해서 펼치고 다시 합치는 방식을 사용.
  * 다른 receptive field를 모아 사용한다는 장점.
  * 계산량을 줄이기 위해 1x1 convolutional layer를 추가로 사용한다. 채널을 줄이는 효과. bottleneck.
  * Auxiliary classifiers
    * 네트워크 중간에 결과를 출력하는 classifier구조가 추가된다
    * backpropagation시 lower layer(초기 layer)까지 gradient전달을 위해서 사용. vanishing gradient 문제 해결을 위함.
    * training때만 사용.

### ResNet
  * degradation 문제
    * 네트워크가 깊어짐에 따라 학습이 안되는 문제.
    * 오버피팅과는 별개의 문제
  * **degradation 해결을 위해 Residual block 사용**
    * 블럭의 입력값(=identity)을 블럭 출력시에 더해준다.
    * Shortcut connection = Skip connection
    * 오차역전파시 전파경로가 2^n 정도로 늘어난다.

## beyond ResNet, 컨셉만

### DenseNet
  * ResNet이 더한다면 DenseNet은 Concatenate한다.
  * Concatenate로 채널이 늘어나므로 한번씩 줄여준다.

### SENet
1. squeeze  
    * global average pooling 사용
      * fully connected를 cnn에서 사용하기 위한 방법.
      * 결과로 1x1xC 형태가 나옴
    
2. Excitation  
    * GAP결과를 FC(W)를 하나 거쳐서 채널간의 관계를 포함하는 어텐션스코어를 얻는다.
    * 이 결과로 feature map을 scale(?)

### EfficientNet
1. deep
    * ResNet

2. wide
    * googLeNet - Inception Block

3. high resolution
    * 높은 해상도의 입력데이터 = 많은 픽셀?

* 1 + 2 + 3 => Compound scaling
* 결과가 좋다

### Deformable convolution
* convolutional layer에서 고정된 receptive field를 가져오지 않고   
  추가로 학습한 offset에 따라 불규칙적인 형태로 가져온다.
  
### backbone으로 쓰기 좋은 모델
1. googLeNet
  * 사용, 구현 복잡하다
  * 성능좋음
2. VGG
  * 쉬움
  * 모델크기큼, 메모리사용량 큼
4. ResNet
  * 쉬움
  * 성능 


# 강의 2

## Semantic segmentation
   * 이미지의 각 픽셀이 어느 레이블인지 판단
   * 같은 레이블들간의 구분은 없다... (instance segmentation에서는 가능)

### 모델 1 - FCN , Fully Convolutional Networks 

* 입력에서 출력까지 미분가능
* convolutional layer로만 구성
* 입력이미지 크기에 상관없이 사용가능

* Fully convolutional layer 사용
    * 1X1
    * 이전 feature map의 픽셀(H,W) 유지, 채널수만 변경
    * 공간정보 유지, spatial information
    * 채널축으로 Flattening하여 벡터화하고 그것들을 stack하여   
      fully connected 하는 것과 유사하다. (그래서 무슨 의미???)
      parameter수는 다른.
         
* Upsampling 사용
    * 해상도를 올리는 것.
    * FCN내에서 입력이미지는 pooling, stride로 저해상도가 된다.
    * receptive field를 확보하기 위해 저해상도로 만든후 다시 upsampling을 하는 방식
    ### 알아볼 방법2가지
    * Transposed convolution
        * 중첩부분(checkboard artifact)이 나와서 더하지 않도록 사이즈 설정을 잘해야한다.
    * Upsampling
        * nearest-neighbor, Biliear + Convolutional
              ?
 
* 초기 레이어에서 receptive field가 작다. 민감한, 디테일 정보
* 뒷쪽 레이어에서 receptive field가 크다. 의미론적, 전체적인 정보
* 여러 layer에서 feature map들을 모아 upsampling 하는 방법을 쓴다. 이 방법이 결과가 좋다
* activation map = feature map에 activation function을 적용한 결과

### 모델 2 - U-net
* 해상도를 절반 낮추고 채널을 절반 늘리는 과정  
* 해상도를 절반 올리고 채널을 절반 감소시키는 과정
* 해상도를 올리는 과정 중에 U자 모양에서 대칭되는 layer에서 feature map을 보내는 connection이 이뤄짐
* 홀수 사이즈 가지지 않도록 주의

### DeepLab
* Conditional Random Fields (CRFs)
    * 픽셀 관련 후처리
* Dilated convolution
    * Atrous convolution
    * weight 적용시 한칸씩 떨어트려서 사용
    * receptive field의 확장
* Depthwise separable convolution
    ?  

   
