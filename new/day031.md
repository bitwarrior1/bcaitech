# 강의

1. 네트워크를 깊게 쌓으면 좋다.
  * 큰 receptive field
  * More capacity and non-linearity (???)

2. 좋은데 문제가 생긴다
  * 계산 규모가 커짐
  * gradient exploding / vanishing 문제
  * degradation : 학습 하락, 오류 증가

### gooLeNet
  * Inception Block이라고하는 여러 kernel_size를 가진 필터를 사용해서 펼치고 다시 합치는 방식을 사용.
  * 다른 receptive field를 모아 사용한다는 장점.
  * 계산량을 줄이기 위해 1x1 convolutional layer를 추가로 사용한다. 채널을 줄이는 효과. bottleneck.
  * Auxiliary classifiers
    * 네트워크 중간에 결과를 출력하는 classifier구조가 추가된다
    * backpropagation시 lower layer(초기 layer)까지 gradient전달을 위해서 사용. vanishing gradient 문제 해결을 위함.
    * training때만 사용.

### ResNet
  * degradation 문제
    * 네트워크가 깊어짐에 따라 학습이 안되는 문제.
    * 오버피팅과는 별개의 문제
  * **degradation 해결을 위해 Residual block 사용**
    * 블럭의 입력값(=identity)을 블럭 출력시에 더해준다.
    * Shortcut connection = Skip connection
    * 오차역전파시 전파경로가 2^n 정도로 늘어난다.

## beyond ResNet, 컨셉만

### DenseNet
  * ResNet이 더한다면 DenseNet은 Concatenate한다.
  * Concatenate로 채널이 늘어나므로 한번씩 줄여준다.

### SENet
1. squeeze  
    * global average pooling 사용
      * fully connected를 cnn에서 사용하기 위한 방법.
      * 결과로 1x1xC 형태가 나옴
    
2. Excitation  
    * GAP결과를 FC(W)를 하나 거쳐서 채널간의 관계를 포함하는 어텐션스코어를 얻는다.
    * 이 결과로 feature map을 scale(?)

### EfficientNet
1. deep
    * ResNet

2. wide
    * googLeNet - Inception Block

3. high resolution
    * 높은 해상도의 입력데이터 = 많은 픽셀?

* 1 + 2 + 3 => Compound scaling
* 결과가 좋다

### Deformable convolution
* convolutional layer에서 고정된 receptive field를 가져오지 않고   
  추가로 학습한 offset에 따라 불규칙적인 형태로 가져온다.
  
### backbone으로 쓰기 좋은 모델
1. googLeNet
  * 사용, 구현 복잡하다
  * 성능좋음
2. VGG
  * 쉬움
  * 모델크기큼, 메모리사용량 큼
4. ResNet
  * 쉬움
  * 성능 
