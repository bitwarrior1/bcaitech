## 강의

* sequential 형 데이터의 문제  
입력값의 크기가 정해지지 않았다.

* Naive sequence model  
  다음 값을 예측하는 모델.
  고려해야 할 이전 값들의 수가 점점 늘어난다.

* Autoregressive model  
  고려해야 할 이전 값들의 수를 고정시킨다.  
  AR-1 : 고려해야 할 이전값이 1개  
  AR-2 : 고려해야 할 이전값이 2개  

* Markov model (first-order autoregressive model)  
  과거 데이터 중에서 몇개만 볼 것인지를 정한다.
  Easy to express the joint distribution...

  단점 : 과거의 많은 데이터를 고려할 수가 없다
  
* Latent autoregressive model
  중간에 hidden state를 추가한다. 이 hidden status에 과거의 데이터가 합쳐져(요약) 있다고 볼수 있다.  
  그리고 다음 time state은 이 하나의 hidden state에만 디펜던트하다.  

  (latent state = hidden state)
  
* 위의 컨셉들은 구현한 것이 RNN  

#### RNN

* 구조
  시간대별로 unroll 하면 입력과 출력이 매우 많은 구조임을 알 수 있다.   
  파라미터를 share하는 mlp와 같은 구조라고 볼 수 있다.

* Short-term dependencies
  먼 과거의 데이터가 학습에 영향을 미치지 못해서 비교적 현재에 가까운 과거데이터에만 dependent하다.  
  이 문제를 해결하기 위해 LSTM이 나오게 되었다.
   
* 왜 학습하기 어려운가?
  반복되는 구조로 되어 있는데,  
  activation function에 의해서 vanishing 또는 exploding 이 일어난다

* LSTM - Long Short-Term Memory

인풋:  
1. 현재시점의 입력. ex) 단어  
2. Previous cell state : 이전 까지의 정보를 취합해 summarize한 정보, 내부에서만 흐르는 정보
3. Previous hidden state : 이전 단계에서의 아웃풋과 동일한 값

아웃풋:  
1. Output (hidden state). 실제 출력값. ex) 다음 단어 예측
2. next cell state
3. next hidden state

게이트 단위로 알면 좋다. 3개의 게이트가 있다.
1. Forget gate : 어떤 정보를 버릴지 결정
2. Input gate : 어떤 정보를 저장할지 결정
1과2를 cell state에 update한다 (Update cell)
3. Output gate : output 계산시 cellstate도 사용한다.

* GRU _ Gated Recurrent Unit  
게이트가 2개뿐이다.  
No cell state, just hidden state.

__추세는 transformer  
sequencial 데이터는 전처리가 많이 필요하다.  
RNN 파라미터가 제법 많이 든다.__
