## 데이터 visualization

1. seaborn  
* matplotlib 감싸고 있는 wrapper.  
* matplotlib보다 더 쉽게 쓸 수 있다.  
* document가 잘 되어있다.  
* tutorial 제공.  

2. plot 종류
* lines  
* scatter  
* reg  
* count  
* bar  
* dist    
...
* FacetGrid 설정

## 통계학 맛보기

* 통계적 모델량은 확률분포를 추정하는 것 (inference)  
기계학습 & 통계학이 공통적으로 추구하는 목표

* 유한한 개수의 데이터만 관찰해서 모집단의 분포를 정확히 맞춘다는 것은 불가능하므로  
근사적으로 확률분포를 추정한다.

* 모수적 방법론 vs 비모수 방법론
  * 모수적 방법론  
    * 선험적 가정, 모수를 추정한다.  (모수의 예로는 평균과 분산이 있다, 분포마다 모수는 다름) 
    
  * 비모수 방법론  
    * 모델의 구조 및 모수의 개수가 유연하게 바뀜
    * 기계학습의 많은 방법론이다.
    
* 표본분포 ≠ 표집분포

* 큰수의 법칙
표본평균이 N이 커지면 모평균과 가까운 수치일 가능성이 커진다
( N = 관측한 데이터 수 )

* 통계량의 확률분포를 표집분포 (예시 >> 표본평균과 표본분산의 확률분포)(**sampling distribution**)라고 부르며  
특히 표본평균의 표집분포는 N이 커질수록 정규분포를 따른다. (= 중심극한 정리)  
N이 커지면 분산은 0에 가까워진다.

## 이해 xX

* 최대가능도 추정법 (MLE) -> 가능 가능성이 높은 모수를 추정하는 방법  
최대가능도 추정법 (MLE)를 통해서 **1)정답에 해당하는 확률분포**와 **2)모델이 추정하는 확률분포**의 거리를 최소화함으로써 모델을 학습시킬 수 있다.   
이 원리는 머신러닝/딥러닝에서 유용하게 사용되고 있다.

  * 분포마다 모수는 다르다. 
  * 주어진 데이터를 기반으로 추정한다.  
  * 데이터 집합이 독립적으로 추출 되었을 대에는 로그가능도로 변환가능하다.
    
  * 최대가능도의 최대화의 의미는 loss함수의 최소화와 일맥상통
  * 로그가능도의 수식에서 필요없는 값(상수취급)을 지우면 loss와 동일함을 확인할 수 있다.
  
  * 확률분포 간의 거리 계산법
    * 확률분포간의 거리의 최소화와 가능도 최대화는 개념적으로 동일하다.

